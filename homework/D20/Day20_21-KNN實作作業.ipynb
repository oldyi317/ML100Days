{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# K-Nearest Neighbors (K-NN)"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "0MRC0e0KhQ0S"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 參考課程實作並在datasets_483_982_spam.csv的資料集中獲得90% 以上的 accuracy (testset)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing the libraries"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "LWd1UlMnhT2s"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd\r\n",
    "import os\r\n",
    "import glob\r\n",
    "import codecs\r\n",
    "import re"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YvGPUQaHhXfL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing the dataset"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "K1VMqkGvhc3-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "dataset = pd.read_csv(r'datasets_483_982_spam.csv', encoding = 'latin-1')\r\n",
    "all_data = []\r\n",
    "\r\n",
    "for content,label in dataset[['v2','v1']].values:\r\n",
    "    if label == 'spam':\r\n",
    "        label = 1\r\n",
    "    else :\r\n",
    "        label = 0\r\n",
    "    all_data.append([content, label])\r\n",
    "all_data = np.array(all_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 取出訓練內文與標註"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "X = all_data[:,0]\r\n",
    "Y = all_data[:,1].astype(np.uint8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print('Training Data Examples : \\n{}'.format(X[:5]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Data Examples : \n",
      "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'\n",
      " 'Ok lar... Joking wif u oni...'\n",
      " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"\n",
      " 'U dun say so early hor... U c already then say...'\n",
      " \"Nah I don't think he goes to usf, he lives around here though\"]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "print('Labeling Data Examples : \\n{}'.format(Y[:5]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Labeling Data Examples : \n",
      "[0 0 1 0 0]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 文字預處理"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from sklearn.metrics import confusion_matrix\r\n",
    "from nltk.corpus import stopwords\r\n",
    "\r\n",
    "import nltk\r\n",
    "\r\n",
    "nltk.download('stopwords')\r\n",
    "\r\n",
    "# Lemmatize with POS Tag\r\n",
    "from nltk.corpus import wordnet\r\n",
    "from nltk.stem import WordNetLemmatizer \r\n",
    "\r\n",
    "lemmatizer = WordNetLemmatizer() \r\n",
    "def get_wordnet_pos(word):\r\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\r\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\r\n",
    "                \"N\": wordnet.NOUN,\r\n",
    "                \"V\": wordnet.VERB,\r\n",
    "                \"R\": wordnet.ADV}\r\n",
    "\r\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\r\n",
    "\r\n",
    "def clean_content(X):\r\n",
    "    # remove non-alphabet characters\r\n",
    "    X_clean = [re.sub('[^a-zA-Z]',' ', x).lower() for x in X]\r\n",
    "    # tokenize\r\n",
    "    X_word_tokenize = [nltk.word_tokenize(x) for x in X_clean]\r\n",
    "    # stopwords_lemmatizer\r\n",
    "    X_stopwords_lemmatizer = []\r\n",
    "    stop_words = set(stopwords.words('english'))\r\n",
    "    for content in X_word_tokenize:\r\n",
    "        content_clean = []\r\n",
    "        for word in content:\r\n",
    "            if word not in stop_words:\r\n",
    "                word = lemmatizer.lemmatize(word, get_wordnet_pos(word))\r\n",
    "                content_clean.append(word)\r\n",
    "        X_stopwords_lemmatizer.append(content_clean)\r\n",
    "    \r\n",
    "    X_output = [' '.join(x) for x in X_stopwords_lemmatizer]\r\n",
    "    \r\n",
    "    return X_output "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tommy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "X = clean_content(X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bag of words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "#max_features是要建造幾個column，會按造字出現的高低去篩選 \r\n",
    "cv=CountVectorizer(max_features = 1500)\r\n",
    "X=cv.fit_transform(X).toarray()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "X.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5572, 1500)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "YvxIPVyMhmKp"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AVzJWAXIhxoC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the K-NN model on the Training set"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "bb6jCOCQiAmP"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\r\n",
    "classifier.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2356,
     "status": "ok",
     "timestamp": 1588492962262,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "e0pFVAmciHQs",
    "outputId": "8cb18c23-669b-452a-9bee-b2f96534f0f5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predicting a new result"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "yyxW5b395mR2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "print('Trainset Accuracy: {}'.format(classifier.score(X_train, y_train)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trainset Accuracy: 0.943459726273278\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2351,
     "status": "ok",
     "timestamp": 1588492962263,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "f8YOXsQy58rP",
    "outputId": "e248f6c5-4613-4a9e-faed-093c46defda1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "print('Testset Accuracy: {}'.format(classifier.score(X_test, y_test)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testset Accuracy: 0.9183856502242153\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predicting the Test set results"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "vKYVQH-l5NpE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "y_pred = classifier.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2345,
     "status": "ok",
     "timestamp": 1588492962263,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "p6VMTb2O4hwM",
    "outputId": "14b859cb-16df-4e5d-894b-3bda8e756d3d"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Making the Confusion Matrix"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "h4Hwj34ziWQW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\r\n",
    "cm = confusion_matrix(y_test, y_pred)\r\n",
    "print(cm)\r\n",
    "accuracy_score(y_test, y_pred)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[949   0]\n",
      " [ 91  75]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9183856502242153"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3505,
     "status": "ok",
     "timestamp": 1588492963427,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "D6bpZwUiiXic",
    "outputId": "ec9468d5-c478-4ffa-ba1c-535eb56d7304"
   }
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO/71HmJztjHpR9Q3DXpRZQ",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "k_nearest_neighbors.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "67019789fca5fcfa1e8e90aaad360222a51a8d5bc2c76577032a0f9c64cf6146"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}