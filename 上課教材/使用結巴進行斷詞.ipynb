{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 瞭解如何使用結巴進行斷詞\n",
    "\n",
    "在先前的課程, 我們熟悉了斷詞運作的基本原理(包含了存在於詞典中的字詞與不存在於詞典中的字詞), 現在讓我們來看看如何使用結巴來進行實際的斷詞操作。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "目前結巴主要支援四種分詞模式:\n",
    "\n",
    "* 精確模式: 精準地斷詞，是基礎的斷詞模式，是合作文本分析使用\n",
    "* 全模式: 將句子中所有可以成詞的詞語都掃瞄出來，速度較快，但不能解決歧義詞\n",
    "* 搜尋引擎模式: 在精確模式的基礎上，對長辭再次切分，提高召回率(Recall)，適合用於搜尋引擎或文本分析\n",
    "* paddle模式: 利用PaddlePaddle深度學習框架，訓練序列標注(雙向GRU)網路模型實現分詞 (欲使用paddle模式，請先安裝paddlepaddle-tiny)\n",
    "\n",
    "本次的學習會著重在前三種分詞模式，對第四種分詞模式有興趣的同學可以參照jieba的GitHub repo來深入了解"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 使用預設詞庫進行四種模式的斷詞\n",
    "\n",
    "* paddle模式: `jieba.cut(str,use_paddle=True)`\n",
    "* 全模式: `jieba.cut(str,cut_all=True)`\n",
    "* 精確模式: `jieba.cut(str, cut_all=False)` (默認為精確模式)\n",
    "* 搜尋引擎模式: `jieba.cut_for_search(str)`\n",
    "\n",
    "以上返回的皆為generator，須透過迴圈(for)來取出斷詞結果"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "'''\r\n",
    "結巴默認的字典為簡體字點，在此我們使用預設字典針對繁體與簡體字串進行斷詞\r\n",
    "'''\r\n",
    "\r\n",
    "import jieba #載入結巴模組\r\n",
    "\r\n",
    "input_traditional_str = '小明碩士畢業於國立臺灣大學，現在在日本東京大學進修深造' #定義須斷詞的繁體字串\r\n",
    "input_simple_str = '小明硕士毕业于国立臺湾大学，现在在日本东京大学进修深造' #定義須斷詞的繁體字串\r\n",
    "\r\n",
    "#使用paddle模式\r\n",
    "jieba.enable_paddle() #啟用paddle模式\r\n",
    "\r\n",
    "trad_words = jieba.cut(input_traditional_str, use_paddle=True)\r\n",
    "sim_words = jieba.cut(input_simple_str, use_paddle=True)\r\n",
    "\r\n",
    "trad_paddle_output = []\r\n",
    "sim_paddle_outupt = []\r\n",
    "\r\n",
    "for word in trad_words:\r\n",
    "    trad_paddle_output.append(word)\r\n",
    "    \r\n",
    "for word in sim_words:\r\n",
    "    sim_paddle_outupt.append(word)\r\n",
    "\r\n",
    "print(f'paddle模式段詞結果:')\r\n",
    "print(trad_paddle_output)\r\n",
    "print(sim_paddle_outupt)\r\n",
    "print('\\n')\r\n",
    "\r\n",
    "#使用全模式\r\n",
    "trad_words = jieba.cut(input_traditional_str, cut_all=True)\r\n",
    "sim_words = jieba.cut(input_simple_str, cut_all=True)\r\n",
    "\r\n",
    "trad_all_output = []\r\n",
    "sim_all_output = []\r\n",
    "\r\n",
    "for word in trad_words:\r\n",
    "    trad_all_output.append(word)\r\n",
    "    \r\n",
    "for word in sim_words:\r\n",
    "    sim_all_output.append(word)\r\n",
    "\r\n",
    "print(f'全模式段詞結果:')\r\n",
    "print(trad_all_output)\r\n",
    "print(sim_all_output)\r\n",
    "print('\\n')\r\n",
    "\r\n",
    "#使用精確模式\r\n",
    "trad_words = jieba.cut(input_traditional_str) #default is accurate mode\r\n",
    "sim_words = jieba.cut(input_simple_str) #default is accurate mode\r\n",
    "\r\n",
    "trad_acc_output = []\r\n",
    "sim_acc_output = []\r\n",
    "\r\n",
    "for word in trad_words:\r\n",
    "    trad_acc_output.append(word)\r\n",
    "    \r\n",
    "for word in sim_words:\r\n",
    "    sim_acc_output.append(word)\r\n",
    "\r\n",
    "print(f'精確模式段詞結果:')\r\n",
    "print(trad_acc_output)\r\n",
    "print(sim_acc_output)\r\n",
    "print('\\n')\r\n",
    "\r\n",
    "#搜尋引擎模式\r\n",
    "trad_words = jieba.cut_for_search(input_traditional_str) \r\n",
    "sim_words = jieba.cut_for_search(input_simple_str) \r\n",
    "\r\n",
    "trad_search_output = []\r\n",
    "sim_search_output = []\r\n",
    "\r\n",
    "for word in trad_words:\r\n",
    "    trad_search_output.append(word)\r\n",
    "    \r\n",
    "for word in sim_words:\r\n",
    "    sim_search_output.append(word)\r\n",
    "\r\n",
    "print(f'搜尋引擎模式段詞結果:')\r\n",
    "print(trad_search_output)\r\n",
    "print(sim_search_output)\r\n",
    "print('\\n')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Installing paddle-tiny, please wait a minute......\n",
      "Import paddle error, please use command to install: pip install paddlepaddle-tiny==1.6.1.Now, back to jieba basic cut......\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "UnboundLocalError",
     "evalue": "local variable 'paddle' referenced before assignment",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33452/1801356965.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#使用paddle模式\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_paddle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#啟用paddle模式\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtrad_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_traditional_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_paddle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\jieba\\_compat.py\u001b[0m in \u001b[0;36menable_paddle\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[1;34m\"Import paddle error, please use command to install: pip install paddlepaddle-tiny==1.6.1.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \"Now, back to jieba basic cut......\")\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;34m'1.6.1'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         default_logger.debug(\"Find your own paddle version doesn't satisfy the minimum requirement (1.6.1), \"\n\u001b[0;32m     41\u001b[0m                              \u001b[1;34m\"please install paddle tiny by 'pip install --upgrade paddlepaddle-tiny', \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'paddle' referenced before assignment"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "由上面的結果可以發現，使用預設詞庫時，在簡體中文的斷詞結果結果比起繁體中文來的好一些。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 返回斷詞list字串\n",
    "\n",
    "* `jieba.lcut(input_string)`\n",
    "\n",
    "* `jieba.lcut_for_search`\n",
    "\n",
    "上述的斷詞法皆是返回generator, 我們也可以使用`lcut`與`lcut_for_search`來取的返回list。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "'''\r\n",
    "結巴默認的字典為簡體字點，在此我們使用預設字典針對繁體與簡體字串進行斷詞\r\n",
    "'''\r\n",
    "\r\n",
    "import jieba #載入結巴模組\r\n",
    "\r\n",
    "input_traditional_str = '小明碩士畢業於國立臺灣大學，現在在日本東京大學進修深造' #定義須斷詞的繁體字串\r\n",
    "input_simple_str = '小明硕士毕业于国立臺湾大学，现在在日本东京大学进修深造' #定義須斷詞的繁體字串\r\n",
    "\r\n",
    "#lcut\r\n",
    "\r\n",
    "trad_words = jieba.lcut(input_traditional_str)\r\n",
    "sim_words = jieba.lcut(input_simple_str)\r\n",
    "\r\n",
    "print(f'lcut斷詞結果:')\r\n",
    "print(trad_words)\r\n",
    "print(sim_words)\r\n",
    "print('\\n')\r\n",
    "\r\n",
    "\r\n",
    "#lcut_for_search\r\n",
    "trad_words = jieba.lcut_for_search(input_traditional_str)\r\n",
    "sim_words = jieba.lcut_for_search(input_simple_str)\r\n",
    "\r\n",
    "print(f'lcut_for_search斷詞結果:')\r\n",
    "print(trad_words)\r\n",
    "print(sim_words)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "lcut斷詞結果:\n",
      "['小明', '碩士', '畢業', '於', '國立', '臺灣大學', '，', '現在', '在', '日本東京大學', '進修', '深造']\n",
      "['小明', '硕士', '毕业', '于', '国立', '臺', '湾', '大学', '，', '现在', '在', '日本东京大学', '进修', '深造']\n",
      "\n",
      "\n",
      "lcut_for_search斷詞結果:\n",
      "['小明', '碩士', '畢業', '於', '國立', '臺灣', '大學', '臺灣大學', '，', '現在', '在', '日本', '東京', '大學', '日本東京大學', '進修', '深造']\n",
      "['小明', '硕士', '毕业', '于', '国立', '臺', '湾', '大学', '，', '现在', '在', '日本', '东京', '大学', '日本东京大学', '进修', '深造']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 使用繁體詞庫\n",
    "`jieba.set_dictionary('dict.txt.big')`\n",
    "\n",
    "在結巴中，其提供另外一個較大的詞庫`dict.txt.big`,此詞庫對繁體中文的支援性較高。\n",
    "範例中已先將字典下載放置在對應的路徑。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "jieba.set_dictionary('dict.txt.big') #load extra dictionary\n",
    "\n",
    "input_traditional_str = '小明碩士畢業於國立臺灣大學，現在在日本東京大學進修深造' #定義須斷詞的繁體字串\n",
    "input_simple_str = '小明硕士毕业于国立臺湾大学，现在在日本东京大学进修深造' #定義須斷詞的繁體字串\n",
    "\n",
    "#使用paddle模式\n",
    "jieba.enable_paddle() #啟用paddle模式\n",
    "\n",
    "trad_words = jieba.cut(input_traditional_str, use_paddle=True)\n",
    "sim_words = jieba.cut(input_simple_str, use_paddle=True)\n",
    "\n",
    "trad_paddle_output = []\n",
    "sim_paddle_outupt = []\n",
    "\n",
    "for word in trad_words:\n",
    "    trad_paddle_output.append(word)\n",
    "    \n",
    "for word in sim_words:\n",
    "    sim_paddle_outupt.append(word)\n",
    "\n",
    "print(f'paddle模式段詞結果:')\n",
    "print(trad_paddle_output)\n",
    "print(sim_paddle_outupt)\n",
    "print('\\n')\n",
    "\n",
    "#使用全模式\n",
    "trad_words = jieba.cut(input_traditional_str, cut_all=True)\n",
    "sim_words = jieba.cut(input_simple_str, cut_all=True)\n",
    "\n",
    "trad_all_output = []\n",
    "sim_all_output = []\n",
    "\n",
    "for word in trad_words:\n",
    "    trad_all_output.append(word)\n",
    "    \n",
    "for word in sim_words:\n",
    "    sim_all_output.append(word)\n",
    "\n",
    "print(f'全模式段詞結果:')\n",
    "print(trad_all_output)\n",
    "print(sim_all_output)\n",
    "print('\\n')\n",
    "\n",
    "#使用精確模式\n",
    "trad_words = jieba.cut(input_traditional_str) #default is accurate mode\n",
    "sim_words = jieba.cut(input_simple_str) #default is accurate mode\n",
    "\n",
    "trad_acc_output = []\n",
    "sim_acc_output = []\n",
    "\n",
    "for word in trad_words:\n",
    "    trad_acc_output.append(word)\n",
    "    \n",
    "for word in sim_words:\n",
    "    sim_acc_output.append(word)\n",
    "\n",
    "print(f'精確模式段詞結果:')\n",
    "print(trad_acc_output)\n",
    "print(sim_acc_output)\n",
    "print('\\n')\n",
    "\n",
    "#搜尋引擎模式\n",
    "trad_words = jieba.cut_for_search(input_traditional_str) \n",
    "sim_words = jieba.cut_for_search(input_simple_str) \n",
    "\n",
    "trad_search_output = []\n",
    "sim_search_output = []\n",
    "\n",
    "for word in trad_words:\n",
    "    trad_search_output.append(word)\n",
    "    \n",
    "for word in sim_words:\n",
    "    sim_search_output.append(word)\n",
    "\n",
    "print(f'搜尋引擎模式段詞結果:')\n",
    "print(trad_search_output)\n",
    "print(sim_search_output)\n",
    "print('\\n')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Paddle enabled successfully......\n",
      "Building prefix dict from /Users/admin/Documents/cupoy/dict.txt.big ...\n",
      "Loading model from cache /var/folders/9f/mkwhwg7d4vz7rp0429_kg43c0000gn/T/jieba.u84c918e3a4b96d5d3945d77bedf6e4ab.cache\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "paddle模式段詞結果:\n",
      "['小明', '碩士', '畢業', '於', '國立臺灣大學', '，現在', '在', '日本', '東京大學', '進修', '深造']\n",
      "['小明', '硕士', '毕业', '于', '国立臺湾大学', '，', '现在', '在', '日本东京大学', '进修', '深造']\n",
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading model cost 1.561 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "全模式段詞結果:\n",
      "['小', '明', '碩士', '畢業', '於', '國立', '臺灣', '臺灣大學', '大學', '，', '現在', '在', '日本', '日本東京大學', '東京', '東京大學', '大學', '進修', '深造']\n",
      "['小', '明', '硕士', '毕业', '于', '国立', '臺', '湾', '大学', '，', '现在', '在', '日本', '日本东京大学', '东京', '东京大学', '大学', '进修', '深造']\n",
      "\n",
      "\n",
      "精確模式段詞結果:\n",
      "['小明', '碩士', '畢業', '於', '國立', '臺灣大學', '，', '現在', '在', '日本東京大學', '進修', '深造']\n",
      "['小明', '硕士', '毕业', '于', '国立', '臺', '湾', '大学', '，', '现在', '在', '日本东京大学', '进修', '深造']\n",
      "\n",
      "\n",
      "搜尋引擎模式段詞結果:\n",
      "['小明', '碩士', '畢業', '於', '國立', '臺灣', '大學', '臺灣大學', '，', '現在', '在', '日本', '東京', '大學', '日本東京大學', '進修', '深造']\n",
      "['小明', '硕士', '毕业', '于', '国立', '臺', '湾', '大学', '，', '现在', '在', '日本', '东京', '大学', '日本东京大学', '进修', '深造']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在此範例中，使用預設字典與使用`dict.txt.big`字典結果並無太大差異，有可能是此範例相較好斷詞。\n",
    "\n",
    "不過之後若是有繁體中文斷詞需求，還是建議使用`dict.txt.big`字典。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 辨識新字詞\n",
    "\n",
    "在先前的課程中我們了解到，當需要斷的詞不在字典裡的時候會使用HMM來計算最大機率的斷詞結果，我們可以來觀察使用HMM與否在斷詞上的差異"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "test_string = \"他来到了网易杭研大厦\" #此舉例為結巴官方repo的例子\n",
    "\n",
    "#使用HMM\n",
    "hmm_words = jieba.cut(test_string, HMM=True) #HMM 參數預設即為True\n",
    "hmm_out = []\n",
    "for word in hmm_words:\n",
    "    hmm_out.append(word)\n",
    "    \n",
    "print(f'HMM output: {hmm_out}')\n",
    "print('\\n')\n",
    "\n",
    "#不使用HMM\n",
    "no_hmm_words = jieba.cut(test_string, HMM=False) #HMM 參數預設即為True\n",
    "no_hmm_out = []\n",
    "for word in no_hmm_words:\n",
    "    no_hmm_out.append(word)\n",
    "    \n",
    "print(f'Non HMM output: {no_hmm_out}')\n",
    "print('\\n')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "HMM output: ['他', '来到', '了', '网易', '杭研', '大厦']\n",
      "\n",
      "\n",
      "Non HMM output: ['他', '来到', '了', '网易', '杭', '研', '大厦']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以發現不使用HMM的斷詞會將`杭研`斷開成`杭`、`研`，因為杭研並沒有在使用的字典中，所以若沒有使用HMM進行斷詞，便無法正確將不存在在詞典的字詞斷開。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 載入自定義詞庫\n",
    "`jieba.load_userdict('userdict.txt')`\n",
    "\n",
    "雖然結巴斷詞可以使用HMM對新字詞(未在詞典中出現的詞)進行斷詞，但我們依然可以自行添加字典，提高對新字詞斷詞的準確度。\n",
    "\n",
    "自定義字典的格式需與結巴字典格式`dict.txt`相同。如下所示\n",
    "\n",
    "```\n",
    "创新办 3 i\n",
    "云计算 5\n",
    "凱特琳 nz\n",
    "台中\n",
    "```\n",
    "\n",
    "其中第一行為自定義的字詞，第二行為詞頻(可省略)，第三行為詞性(可省略)。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#使用結巴的繁體中文辭典對台語進行斷詞\n",
    "\n",
    "jieba.set_dictionary('dict.txt.big') #load extra dictionary\n",
    "\n",
    "test_string = '親愛的媽媽請你毋通煩惱我原諒我行袂開跤我欲去對抗袂當原諒的人'\n",
    "\n",
    "words = jieba.cut(test_string, cut_all=False) #使用精確模式\n",
    "out = []\n",
    "\n",
    "for word in words:\n",
    "    out.append(word)\n",
    "    \n",
    "print(out)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building prefix dict from /Users/admin/Documents/cupoy/dict.txt.big ...\n",
      "Loading model from cache /var/folders/9f/mkwhwg7d4vz7rp0429_kg43c0000gn/T/jieba.u84c918e3a4b96d5d3945d77bedf6e4ab.cache\n",
      "Loading model cost 1.515 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['親愛', '的', '媽媽', '請', '你', '毋通', '煩惱', '我', '原諒', '我行', '袂', '開跤', '我', '欲', '去', '對抗', '袂', '當', '原諒', '的', '人']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以發現結巴使用繁體字庫對與的斷詞還是有些地方會不合理，像是將`袂當`斷成了`袂`與`當`，以及將`我行袂開跤`斷成`我行`, `袂`, `開跤`。 這時我們可以使用自定義的詞庫將這樣的詞語加入字典中。\n",
    "\n",
    "```\n",
    "行袂開跤 2 v\n",
    "袂當 4 d\n",
    "袂記 4 v\n",
    "袂有 4 \n",
    "唱著 \n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "jieba.set_dictionary('dict.txt.big') #load extra dictionary\n",
    "jieba.load_userdict(\"userdict.txt\") #load user defined dictionary\n",
    "\n",
    "test_string = '親愛的媽媽請你毋通煩惱我原諒我行袂開跤我欲去對抗袂當原諒的人'\n",
    "\n",
    "words = jieba.cut(test_string, cut_all=False) #使用精確模式\n",
    "out = []\n",
    "\n",
    "for word in words:\n",
    "    out.append(word)\n",
    "    \n",
    "print(out)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building prefix dict from /Users/admin/Documents/cupoy/dict.txt.big ...\n",
      "Loading model from cache /var/folders/9f/mkwhwg7d4vz7rp0429_kg43c0000gn/T/jieba.u84c918e3a4b96d5d3945d77bedf6e4ab.cache\n",
      "Loading model cost 1.458 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['親愛', '的', '媽媽', '請', '你', '毋通', '煩惱', '我', '原諒', '我', '行袂開跤', '我', '欲', '去', '對抗', '袂當', '原諒', '的', '人']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以發現這時就能將`袂當`正確斷成`袂當`以及`我行袂開跤`斷成`我`, `行袂開跤`。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 動態調整字典\n",
    "\n",
    "* 加入字典:`add_word(word, freq=None, tag=None)`\n",
    "\n",
    "* 從字典中刪除: `del_word(word)`\n",
    "\n",
    "* 調整字詞詞頻: `suggest_freq(segment, tune=True)`\n",
    "\n",
    "除了使用上述的直接載入自定義字典外，我們也可以在程式中動態的修改想要的斷詞與字詞的詞頻。\n",
    "\n",
    "調整詞頻可能會使原本無法(可以)被斷詞的字詞，使其可以(無法)被分出來。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "jieba.add_word('國立臺灣大學') #動態加入字詞\n",
    "\n",
    "input_str = '小明碩士畢業於國立臺灣大學，現在在日本東京大學進修深造' #定義須斷詞的字串\n",
    "\n",
    "#使用精確模式\n",
    "words = jieba.cut(input_str) #default is accurate mode\n",
    "acc_output = []\n",
    "\n",
    "for word in words:\n",
    "    acc_output.append(word)\n",
    "\n",
    "print(f'精確模式段詞結果:')\n",
    "print(acc_output)\n",
    "print('\\n')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "精確模式段詞結果:\n",
      "['小明', '碩士', '畢業', '於', '國立臺灣大學', '，', '現在', '在', '日本東京大學', '進修', '深造']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "import jieba #載入結巴模組\n",
    "\n",
    "jieba.suggest_freq('國立臺灣大學', True) #動態調整詞頻\n",
    "\n",
    "input_str = '小明碩士畢業於國立臺灣大學，現在在日本東京大學進修深造' #定義須斷詞的字串\n",
    "\n",
    "#使用精確模式\n",
    "words = jieba.cut(input_str) #default is accurate mode\n",
    "acc_output = []\n",
    "\n",
    "for word in words:\n",
    "    acc_output.append(word)\n",
    "\n",
    "print(f'精確模式段詞結果:')\n",
    "print(acc_output)\n",
    "print('\\n')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "精確模式段詞結果:\n",
      "['小明', '碩士', '畢業', '於', '國立臺灣大學', '，', '現在', '在', '日本東京大學', '進修', '深造']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "在原來的例子中，精確模式會將`國立台灣大學`斷詞成`國立`, `台灣大學`。 當我們動態加入字詞或調整詞頻後，就可以將`國立台灣大學`斷成`國立台灣大學`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 詞性標註\n",
    "\n",
    "`jieba.posseg.cut(string)`\n",
    "\n",
    "結巴也可以對斷詞進行詞性的標注，但這邊請注意，詞性標注的結果跟斷詞系統所使用的字典語料庫有關，結巴使用人民日報進行訓練的結果，不一定會符合使用者所應用的領域。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "input_str = '小明碩士畢業於國立臺灣大學，現在在日本東京大學進修深造' #定義須斷詞的字串\n",
    "\n",
    "jieba.set_dictionary('./dict.txt.big') #load extra dictionary\n",
    "\n",
    "words = pseg.cut(input_str)\n",
    "\n",
    "for word, flag in words:\n",
    "    print(word, flag)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building prefix dict from /Users/admin/Documents/cupoy/dict.txt.big ...\n",
      "Loading model from cache /var/folders/9f/mkwhwg7d4vz7rp0429_kg43c0000gn/T/jieba.u84c918e3a4b96d5d3945d77bedf6e4ab.cache\n",
      "Loading model cost 1.452 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "小明 nr\n",
      "碩士 n\n",
      "畢業 n\n",
      "於 nr\n",
      "國立 b\n",
      "臺灣大學 nt\n",
      "， x\n",
      "現在 t\n",
      "在 p\n",
      "日本東京大學 nt\n",
      "進修 v\n",
      "深造 v\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "關於詞定代號的意義，同學可以參考延伸閱讀。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 取出斷詞位置\n",
    "\n",
    "`jieba.tokenize(u'content')`\n",
    "\n",
    "有時我們會需要斷詞的字詞起點與終點位置，我們也可以使用結巴的`tokenize`達到需求。 在使用前需要把輸入字串轉為unicode。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "import jieba\n",
    "\n",
    "input_str = u'小明碩士畢業於國立臺灣大學，現在在日本東京大學進修深造' #在此將字串轉為unicode\n",
    "\n",
    "jieba.set_dictionary('./dict.txt.big') #load extra dictionary\n",
    "words = jieba.tokenize(input_str)\n",
    "\n",
    "for tk in words:\n",
    "    print(f'word:{tk[0]}, start:{tk[1]}, end:{tk[2]}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building prefix dict from /Users/admin/Documents/cupoy/dict.txt.big ...\n",
      "Loading model from cache /var/folders/9f/mkwhwg7d4vz7rp0429_kg43c0000gn/T/jieba.u84c918e3a4b96d5d3945d77bedf6e4ab.cache\n",
      "Loading model cost 1.451 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "word:小明, start:0, end:2\n",
      "word:碩士, start:2, end:4\n",
      "word:畢業, start:4, end:6\n",
      "word:於, start:6, end:7\n",
      "word:國立, start:7, end:9\n",
      "word:臺灣大學, start:9, end:13\n",
      "word:，, start:13, end:14\n",
      "word:現在, start:14, end:16\n",
      "word:在, start:16, end:17\n",
      "word:日本東京大學, start:17, end:23\n",
      "word:進修, start:23, end:25\n",
      "word:深造, start:25, end:27\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 關鍵字提取\n",
    "\n",
    "`jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())`\n",
    "\n",
    "* sentence: 輸入字串(文本)\n",
    "\n",
    "* topK: 返回k個tf-idf權重最大的關鍵字詞\n",
    "\n",
    "* withWeight: 是否一併返回關鍵詞權重值\n",
    "\n",
    "* allowPOS: 僅包括指定詞性的詞"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "\n",
    "jieba.set_dictionary('dict.txt.big')\n",
    "\n",
    "input_str = '小明碩士畢業於國立臺灣大學，現在在日本東京大學進修深造' \n",
    "\n",
    "tags = jieba.analyse.extract_tags(input_str, 5, withWeight=True)\n",
    "print(tags)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building prefix dict from /Users/admin/Documents/cupoy/dict.txt.big ...\n",
      "Loading model from cache /var/folders/9f/mkwhwg7d4vz7rp0429_kg43c0000gn/T/jieba.u84c918e3a4b96d5d3945d77bedf6e4ab.cache\n",
      "Loading model cost 1.792 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('碩士', 1.3283075003222222), ('畢業', 1.3283075003222222), ('國立', 1.3283075003222222), ('臺灣大學', 1.3283075003222222), ('現在', 1.3283075003222222)]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "67019789fca5fcfa1e8e90aaad360222a51a8d5bc2c76577032a0f9c64cf6146"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}